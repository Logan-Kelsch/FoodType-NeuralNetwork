{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COSC 311\n",
    "Homework 2 Code\n",
    "Logan Kelsch\n",
    "Food Type Neural Network\n",
    "'''\n",
    "#IMPORT LIBRARIES AND FUNCTIONS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#load data from csv\n",
    "data = pd.read_csv('FoodTypeDataset.csv',header=None)\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#sample count confirmation\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "#target class counts for developing class weights\n",
    "last_column = data.iloc[:, -1].values\n",
    "unique, counts = np.unique(last_column, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "#creating variable for use in .compile for model\n",
    "classWeights = counts\n",
    "\n",
    "# Separate features and target into X and y\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "#Encoding target\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "\n",
    "#split data for training and examination\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#learning rate function for training optimization\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',#watch this value\n",
    "    factor=0.90, #reduce to this percent when\n",
    "    patience=32, #value hasn't improved in this long\n",
    "    min_lr=1e-6 #and stop here\n",
    ")\n",
    "\n",
    "#different optimizers I used during training\n",
    "opt1 = SGD(learning_rate=0.01)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "#commented out opt3 - contains deleted function\n",
    "#opt3 = SGD(learning_rate=lr_schedule)\n",
    "#this was the final optimizer I used.\n",
    "opt4 = SGD(learning_rate=0.005, momentum=0.98)\n",
    "\n",
    "#early stopping function to cut training short if the TRAINING recall stops improving\n",
    "#this was decided as recall was the slowest to improve overall, and therefore\n",
    "#was a great indicator of knowing when training is no longer productive\n",
    "early_stopping = EarlyStopping(monitor='recall', patience=128, mode='max', restore_best_weights=True)\n",
    "\n",
    "#function to construct model in one go\n",
    "def build_model():\n",
    "    '''\n",
    "        maximized normalization in this model to combat agressive\n",
    "        overfitting, as there are many features and classes, \n",
    "        and less samples to create strong model understandings\n",
    "        used this approach as we were looking strictly for best validation\n",
    "        performance output\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.6),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        #20 classes in output layer\n",
    "        tf.keras.layers.Dense(20, activation='softmax')\n",
    "    ])\n",
    "    #custom metrics I wanted for verbose output\n",
    "    met = ['precision','recall','accuracy']\n",
    "    #compile all\n",
    "    model.compile(optimizer=opt4,\n",
    "                  loss='categorical_crossentropy'\n",
    "                  ,metrics=met)\n",
    "    return model\n",
    "\n",
    "#considering the LR function and earlystopping\n",
    "#I set epochs very high as I knew it would not get this far\n",
    "epochs = 2000\n",
    "\n",
    "#build and fit model\n",
    "model = build_model()\n",
    "#record history\n",
    "history = model.fit(X_train, y_train, epochs=epochs,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test), \\\n",
    "                        callbacks=[early_stopping], batch_size=20, \\\n",
    "                            class_weight=classWeights)\n",
    "\n",
    "#visualize model performance using matpltlib\n",
    "\n",
    "#define for proper formatting\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "#loss output\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "#accuracy output\n",
    "plt.plot(epochs, history.history['accuracy'], 'y', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "#precision output\n",
    "plt.plot(epochs, history.history['precision'], 'y', label='Training Precision')\n",
    "plt.plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "#recall output\n",
    "plt.plot(epochs, history.history['recall'], 'y', label='Training Recall')\n",
    "plt.plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#predicting the test set results\n",
    "y_true = np.argmax(y_test, axis=1)  #this avoided bugs for me in my personal \n",
    "                                    #project, am unsure what it does, and kept it\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)  #prediction values for all data\n",
    "\n",
    "#confusion matrix creation\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#confusion matrix output\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=range(20), yticklabels=range(20))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 5-Class Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model saving for future use\n",
    "model.save('model-1.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
