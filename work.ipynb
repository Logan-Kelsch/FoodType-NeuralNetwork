{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('FoodTypeDataset.csv',header=None)\n",
    "\n",
    "#testing random feature drops\n",
    "#data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='CLASS')\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "#MARKET HOURS!\n",
    "last_column = data.iloc[:, -1].values\n",
    "unique, counts = np.unique(last_column, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(class_counts)\n",
    "\n",
    "\n",
    "#cw = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3, 4: weight_for_4}\n",
    "classWeights = counts\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "#SMOTE OVERSAMPLING________________\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X, y = smote.fit_resample(X,y)\n",
    "#print('\\n[PRE-SPLIT] Resampled Data size:',X.size,'--',y.size)\n",
    "\n",
    "#__________________________________\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#42, stratify=y)\n",
    "\n",
    "# one-hot encode ? \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, num_classes=4)\n",
    "#y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "#RESAMPLED DATA- POST SPLIT---------------------------------------------------------\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "#print('\\nResampled Data size:',X_resampled.size)\n",
    "\n",
    "#BUILD THE NEURAL NETWORK MODEL-------------------------------------------------------\n",
    "\n",
    "#CUSTOM CALLBACK FOR PRECISION RATIO TRAINING VS VALIDATION--------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#END CUSTOM LOSSES__________________________________________________________________________________________\n",
    "\n",
    "#LEARNING RATES____________________________________________________________________________________________\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.1,\n",
    "    decay_steps=24,\n",
    "    decay_rate=.99,\n",
    "    staircase=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.90, \n",
    "    patience=32, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "opt1 = SGD(learning_rate=0.01)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "opt4 = SGD(learning_rate=0.005, momentum=0.98)\n",
    "\n",
    "#BUILD AND LOAD MODEL__________________________________________________________________________________________\n",
    "early_stopping = EarlyStopping(monitor='recall', patience=128, mode='max', restore_best_weights=True)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([#currently 17 total features\n",
    "        #tf.keras.layers.Input(shape=(X_train[1],)),\n",
    "        tf.keras.layers.Dense(512),#kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.6),\n",
    "        tf.keras.layers.Dense(256),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128),#,  kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64),#, kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(20, activation='softmax')\n",
    "    ])\n",
    "    #AUC=tf.keras.metrics.AUC(curve='PR')\n",
    "    met = ['precision','recall','accuracy']\n",
    "    model.compile(optimizer=opt4,\n",
    "                  loss='categorical_crossentropy'\n",
    "                  ,metrics=met)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('multi_test1.keras')\n",
    "    met = ['accuracy','precision','recall']\n",
    "    loaded_model.compile()\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "#loaded_model = load_model()\n",
    "history = model.fit(X_train, y_train, epochs=epochs,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test), callbacks=[reduce_lr,early_stopping], batch_size=20)\n",
    "                    #class_weight=cw, callbacks=[metric_callback])\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "plt.plot(epochs, history.history['accuracy'], 'y', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# AUC\n",
    "plt.plot(epochs, history.history['AUC'], 'y', label='Training AUC')\n",
    "plt.plot(epochs, history.history['val_AUC'], 'r', label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# PRECISION\n",
    "plt.plot(epochs, history.history['precision'], 'y', label='Training Precision')\n",
    "plt.plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# RECALL\n",
    "plt.plot(epochs, history.history['recall'], 'y', label='Training Recall')\n",
    "plt.plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# TPR\n",
    "'''\n",
    "TPR = history.history['TruePositives']/(history.history['TruePositives']+history.history['TrueNegatives'])\n",
    "val_TPR = history.history['val_TruePositives']/(history.history['val_TruePositives']+history.history['val_TrueNegatives'])\n",
    "plt.plot(epochs, TPR, 'y', label='Training TPR')\n",
    "plt.plot(epochs, val_TPR, 'r', label='Validation TPR')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "#predicting the test set results\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices if needed\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)  # Predictions to class indices\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=range(20), yticklabels=range(20))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 5-Class Classification')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "#model.save('epoch15k.keras')\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model-1.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
